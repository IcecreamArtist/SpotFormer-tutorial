{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc6ef80",
   "metadata": {},
   "source": [
    "# Tutorial 4: Spot-level differential abundance analysis for AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0184d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import tifffile\n",
    "from model.datasets.data_module import DataModule\n",
    "from model.datasets.pretrain_dataset import (SpatialRadiusDataset, \n",
    "                                             my_collate_fn)\n",
    "import cv2\n",
    "from argparse import Namespace\n",
    "from skimage import filters, measure\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from pytorch_lightning import seed_everything, Trainer\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "import argparse\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import meld\n",
    "import joblib\n",
    "from step1_pretrain import Omics, EpochCallback\n",
    "from meld_analysis_2766g import *\n",
    "import sklearn\n",
    "from embedding_gen import *\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Set matplotlib parameters\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f4c2a",
   "metadata": {},
   "source": [
    "## Initialize configs and fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# Directly write parameters in config\n",
    "args = Namespace(\n",
    "    freeze_bert=False,\n",
    "    emb_dim=256,  # 128, 256\n",
    "    num_workers=16,\n",
    "    learning_rate=1e-3,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.05,\n",
    "    batch_size=50,\n",
    "    experiment_name=\"\",\n",
    "    lambda_1=1.,\n",
    "    seed=42,\n",
    "    data_pct=1,\n",
    "    mask_ratio=0,\n",
    "    radius=200,\n",
    "    scale=1,\n",
    "    ckpt_path=os.path.join(BASE_DIR, 'model_checkpoint.ckpt'),\n",
    "    dataset_name='AD_2766g_m9723',\n",
    "    fold='fold_1',\n",
    "    max_points=100,\n",
    "    label_type='nucleus',\n",
    "    linear_hidden_dim=None,\n",
    "    config=os.path.join(BASE_DIR, '../../configs/bert_config.json'),\n",
    "    debug=False,  # whether to run debug mode\n",
    "    output_dir=\"full_slice_results\",  # output directory\n",
    "    force_reprocess=False,  # whether to force reprocess data\n",
    "    hidden_dim=768,\n",
    "    output_dim=768,\n",
    "    pca_path=None,\n",
    "    norm_path=None,\n",
    "    zoom_area=None\n",
    ")\n",
    "\n",
    "print(f\"Task: {args.task}\")\n",
    "print(f\"Dataset: {args.dataset_name}\")\n",
    "print(f\"Fold: {args.fold}\")\n",
    "print(f\"Batch size: {args.batch_size}\")\n",
    "print(f\"Learning rate: {args.learning_rate}\")\n",
    "print(f\"Output directory: {args.output_dir}\")\n",
    "\n",
    "def parse_zoom_area(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    try:\n",
    "        values = [float(x) for x in value.strip('[]').split(',')]\n",
    "        if len(values) != 4:\n",
    "            raise ValueError(\"Invalid zoom area format: {value}, error: {e}\")\n",
    "        return values\n",
    "    except Exception as e:\n",
    "        raise argparse.ArgumentTypeError(f\"Invalid zoom area format: {value}, error: {e}\")\n",
    "\n",
    "args.gpus = 1\n",
    "debug_mode = args.debug  # debug mode flag\n",
    "print('current mode: debug==', debug_mode)\n",
    "print('current mode: force_reprocess==', args.force_reprocess)\n",
    "# create output directory\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(args.output_dir, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(args.output_dir, \"data\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c86dd",
   "metadata": {},
   "source": [
    "## Generate spot embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d49cd",
   "metadata": {},
   "source": [
    "### Loading PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.pca_path is not None:\n",
    "    # loading PCA model\n",
    "    pca = joblib.load(args.pca_path)\n",
    "    # loading normalization parameters\n",
    "    min_vals, max_vals = joblib.load(args.norm_path)\n",
    "else:\n",
    "    pca = PCA(n_components=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1ef7b",
   "metadata": {},
   "source": [
    "## Loading pre-trained SpotFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "seed_everything(args.seed)\n",
    "\n",
    "# loading model\n",
    "model = Omics.load_from_checkpoint(**args.__dict__)\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7777b28",
   "metadata": {},
   "source": [
    "## Loading datasets and generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.mask_function = 'random'\n",
    "\n",
    "if args.dataset_name == 'seqfish':\n",
    "    train_split = SEQFISH_FOLDS[args.fold]['train']\n",
    "    val_split = SEQFISH_FOLDS[args.fold]['val']\n",
    "elif args.dataset_name == 'merfish':\n",
    "    train_split = MERFISH_FOLDS[args.fold]['train']\n",
    "    val_split = MERFISH_FOLDS[args.fold]['val']\n",
    "elif args.dataset_name == 'mop1':\n",
    "    train_split = MOP_FOLDS1[args.fold]['train']\n",
    "    val_split = MOP_FOLDS1[args.fold]['val']\n",
    "elif args.dataset_name == 'AD_64g_m9721' or args.dataset_name == 'AD_64g_m9781':\n",
    "    train_split = AD_FOLDS[args.fold]['train']\n",
    "    val_split = AD_FOLDS[args.fold]['val']\n",
    "elif args.dataset_name == 'xenium_hbc1':\n",
    "    train_split = XENIUM_HBC_FOLDS1[args.fold]['train']\n",
    "    val_split = XENIUM_HBC_FOLDS1[args.fold]['val']\n",
    "elif args.dataset_name == 'cosmx_lung5_rep1':\n",
    "    train_split = COSMX_FOLDS51[args.fold]['train']\n",
    "    val_split = COSMX_FOLDS51[args.fold]['val']\n",
    "else:\n",
    "    raise ValueError(f\"Dataset {args.dataset_name} not supported\")\n",
    "\n",
    "# file prefix\n",
    "file_prefix = f\"{args.label_type}_{args.dataset_name}_{args.fold}\"\n",
    "adata_path = os.path.join(args.output_dir, \"data\", f\"{file_prefix}_full_slice.h5ad\")\n",
    "\n",
    "\n",
    "# create data module\n",
    "datamodule = DataModule(SpatialRadiusDataset, my_collate_fn,\n",
    "                        args.data_pct, args.batch_size, \n",
    "                        args.num_workers, radius=args.radius, \n",
    "                        mask_ratio=args.mask_ratio, mask_function=args.mask_function,\n",
    "                        dataset_name=args.dataset_name, max_points=args.max_points,\n",
    "                        train_split=train_split, val_split=val_split,\n",
    "                        label_type=args.label_type)\n",
    "\n",
    "train_dataloader = datamodule.inference_train_dataloader()\n",
    "val_dataloader = datamodule.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b06f1c",
   "metadata": {},
   "source": [
    "### Generate embeddings and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process and save data\n",
    "train_reps, _, _, train_indices = process_and_save_data(\n",
    "    train_dataloader, model, \"train\", debug=debug_mode)\n",
    "val_reps, _, _, val_indices = process_and_save_data(\n",
    "    val_dataloader, model, \"val\", debug=debug_mode)\n",
    "\n",
    "# load full dataset\n",
    "full_adata, train_adata, val_adata, train_slice, val_slice = load_dataset(\n",
    "    args.dataset_name, args.fold, train_indices, val_indices, debug=debug_mode)\n",
    "\n",
    "# extract coordinates\n",
    "train_coords = train_adata[['x', 'y']].values\n",
    "val_coords = val_adata[['x', 'y']].values\n",
    "\n",
    "# create AnnData objects for training and validation\n",
    "train_anndata = AnnData(obs=train_adata)\n",
    "train_anndata.obsm['spatial'] = train_coords\n",
    "train_anndata.obs['set_type'] = 'train'\n",
    "# use PCA to reduce dimensions to 3D\n",
    "if args.pca_path is not None:\n",
    "    reduced_reps = pca.transform(train_reps)\n",
    "    min_vals = reduced_reps.min(axis=0)\n",
    "    max_vals = reduced_reps.max(axis=0)\n",
    "    normalized_reps = (reduced_reps - min_vals) / (max_vals - min_vals)\n",
    "    train_anndata.obsm['X_pca'] = normalized_reps\n",
    "else:\n",
    "    reduced_reps = pca.fit_transform(train_reps)\n",
    "    min_vals = reduced_reps.min(axis=0)\n",
    "    max_vals = reduced_reps.max(axis=0)\n",
    "    normalized_reps = (reduced_reps - min_vals) / (max_vals - min_vals)\n",
    "    train_anndata.obsm['X_pca'] = normalized_reps\n",
    "    pca_path = os.path.join(args.output_dir, \"data\", f\"{file_prefix}_pca.pkl\")\n",
    "    joblib.dump(pca, pca_path)\n",
    "\n",
    "val_anndata = AnnData(obs=val_adata)\n",
    "val_anndata.obsm['spatial'] = val_coords\n",
    "val_anndata.obs['set_type'] = 'val'\n",
    "reduced_reps = pca.transform(val_reps)\n",
    "min_vals = reduced_reps.min(axis=0)\n",
    "max_vals = reduced_reps.max(axis=0)\n",
    "normalized_reps = (reduced_reps - min_vals) / (max_vals - min_vals)\n",
    "val_anndata.obsm['X_pca'] = normalized_reps\n",
    "\n",
    "# concatenate training and validation data\n",
    "full_adata = ad.concat([train_anndata, val_anndata], join=\"outer\")\n",
    "\n",
    "\n",
    "# save full AnnData object\n",
    "print(f\"Saving full AnnData object to {adata_path}...\")\n",
    "full_adata.obs['nucleus'] = full_adata.obs['nucleus'].astype(str)\n",
    "full_adata.write_h5ad(adata_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455cf18",
   "metadata": {},
   "source": [
    "## Initialize configs and fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly write parameters in config\n",
    "args = Namespace(\n",
    "    data_paths=[\n",
    "        '/path/to/control_sample1.h5ad',\n",
    "        '/path/to/AD_sample1.h5ad'\n",
    "    ],  # change to actual h5ad file path\n",
    "    conditions=['control', 'AD'],  # corresponding to each dataset condition label\n",
    "    plaque_sample='AD_2766g_m9723',\n",
    "    plaque_image_path='/path/to/plaque_protein_image.tif',  # change to actual tif file path\n",
    "    output_dir='./results',\n",
    "    target_sample_size=200000,\n",
    "    beta=67,\n",
    "    knn=7,\n",
    "    min_plaque_area=100,\n",
    "    surrounding_kernel_size=9,\n",
    "    surrounding_iterations=15\n",
    ")\n",
    "\n",
    "print(f\"Processing {len(args.data_paths)} datasets\")\n",
    "print(f\"Conditions: {args.conditions}\")\n",
    "print(f\"Plaque sample: {args.plaque_sample}\")\n",
    "print(f\"Output directory: {args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a3843",
   "metadata": {},
   "source": [
    "## Preprocess and sample spot embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ec73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MELD Analysis with Protein Plaque Distance Analysis ===\")\n",
    "print(f\"Data paths: {args.data_paths}\")\n",
    "print(f\"Conditions: {args.conditions}\")\n",
    "print(f\"Plaque sample: {args.plaque_sample}\")\n",
    "print(f\"Output directory: {args.output_dir}\")\n",
    "\n",
    "# Create output directories\n",
    "figures_dir, data_dir = create_output_dirs(args.output_dir)\n",
    "\n",
    "# Process and sample spot embeddings\n",
    "sample_adata = process_and_sample_datasets(args.data_paths, args.conditions, \n",
    "                                            args.target_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634f62b",
   "metadata": {},
   "source": [
    "## Meld analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa06f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MELD analysis\n",
    "sample_densities, sample_likelihoods = run_meld_analysis(sample_adata, args.beta, args.knn)\n",
    "\n",
    "# Calculate AD likelihood (using first experimental sample as reference)\n",
    "experimental_samples = [col for col in sample_likelihoods.columns if 'control' not in col.lower()]\n",
    "if experimental_samples:\n",
    "    sample_adata.obs['AD_likelihood'] = sample_likelihoods[experimental_samples].mean(axis=1).values\n",
    "else:\n",
    "    print(\"Warning: No experimental samples found for AD likelihood calculation\")\n",
    "    sample_adata.obs['AD_likelihood'] = sample_likelihoods.mean(axis=1).values\n",
    "\n",
    "# Save MELD results\n",
    "meld_results_path = os.path.join(data_dir, 'sample_adata_with_meld.h5ad')\n",
    "sample_adata.write_h5ad(meld_results_path)\n",
    "print(f\"MELD results saved to: {meld_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f452d7b",
   "metadata": {},
   "source": [
    "## Visualize Meld analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MELD results\n",
    "colors = ['#479EA2', '#C16AAF']\n",
    "plot_and_save(\n",
    "    lambda **kwargs: sc.pl.umap(sample_adata, color=['condition'], \n",
    "                                palette=colors, **kwargs),\n",
    "    'umap_condition', figures_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f63b9",
   "metadata": {},
   "source": [
    "## Process plaque data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== Processing Plaque Analysis for {args.plaque_sample} ===\")\n",
    "\n",
    "# Get subset for plaque analysis\n",
    "sample_adata_subset = sample_adata[sample_adata.obs['dataset'] == args.plaque_sample].copy()\n",
    "\n",
    "if len(sample_adata_subset) == 0:\n",
    "    print(f\"Warning: No data found for sample {args.plaque_sample}\")\n",
    "\n",
    "# Process plaque image\n",
    "abeta_img, abeta_gray, binary_mask_closed, labeled_mask, plaque_df = process_plaque_image(\n",
    "    args.plaque_image_path, args.min_plaque_area)\n",
    "\n",
    "# Create region masks\n",
    "region_mask = create_region_masks(binary_mask_closed, args.surrounding_kernel_size, \n",
    "                                args.surrounding_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cec57d",
   "metadata": {},
   "source": [
    "## Calculate distances to plaques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances to plaques\n",
    "min_distances, nearest_plaque_ids = calculate_distances_to_plaques(\n",
    "    sample_adata_subset, plaque_df, abeta_img)\n",
    "\n",
    "# Add results to subset\n",
    "sample_adata_subset.obs['nearest_plaque_distance'] = min_distances\n",
    "sample_adata_subset.obs['nearest_plaque_id'] = nearest_plaque_ids\n",
    "\n",
    "# Set invalid distances\n",
    "sample_adata_subset.obs.loc[min_distances == float('inf'), 'nearest_plaque_id'] = -1\n",
    "sample_adata_subset.obs.loc[min_distances == float('inf'), 'nearest_plaque_distance'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858bcb0",
   "metadata": {},
   "source": [
    "## Analysis AD likelihood by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1645f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze AD likelihood by distance\n",
    "analysis_results = analyze_ad_likelihood_by_distance(sample_adata_subset, figures_dir)\n",
    "\n",
    "# Update original dataset\n",
    "mask = sample_adata.obs['dataset'] == args.plaque_sample\n",
    "sample_adata.obs.loc[mask, 'nearest_plaque_distance'] = min_distances\n",
    "sample_adata.obs.loc[mask, 'nearest_plaque_id'] = nearest_plaque_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d0bdf",
   "metadata": {},
   "source": [
    "## Save final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59388535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "final_results_path = os.path.join(data_dir, 'sample_adata_with_plaque_analysis.h5ad')\n",
    "sample_adata.write_h5ad(final_results_path)\n",
    "print(f\"Final results saved to: {final_results_path}\")\n",
    "\n",
    "# Save plaque data\n",
    "plaque_df.to_csv(os.path.join(data_dir, f'plaque_information_{args.plaque_sample}.csv'), \n",
    "                    index=False)\n",
    "\n",
    "# Save analysis results\n",
    "if analysis_results:\n",
    "    with open(os.path.join(data_dir, f'distance_analysis_results_{args.plaque_sample}.json'), 'w') as f:\n",
    "        json.dump(analysis_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n=== Analysis Complete ===\")\n",
    "print(f\"Final dataset shape: {sample_adata.shape}\")\n",
    "print(f\"Plaques identified: {len(plaque_df)}\")\n",
    "if analysis_results and analysis_results['significant']:\n",
    "    print(f\"âœ“ Significant difference found (p={analysis_results['p_value']:.6f})\")\n",
    "    print(f\"  Close to plaque mean: {analysis_results['close_mean']:.4f}\")\n",
    "    print(f\"  Far from plaque mean: {analysis_results['far_mean']:.4f}\")\n",
    "else:\n",
    "    print(\"No significant difference found between close/far groups\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
