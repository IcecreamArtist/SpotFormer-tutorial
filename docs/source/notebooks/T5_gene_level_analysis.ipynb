{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3517b9e1",
   "metadata": {},
   "source": [
    "# Tutorial 5: Generate context-aware gene embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "from model.constants import *\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import helper\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from omics.constants import *\n",
    "import h5py\n",
    "from pytorch_lightning.callbacks import (EarlyStopping, LearningRateMonitor,\n",
    "                                         ModelCheckpoint, Callback)\n",
    "from math import exp\n",
    "import random\n",
    "import h5py\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "import hashlib\n",
    "import anndata as ad\n",
    "from model.emb_gen.pretrain import Omics\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from scipy.spatial import cKDTree\n",
    "from omics.baseline.imputation.preprocess.spot_input_gen import load_spot_data_with_full_info\n",
    "from step1_cell_specific_gene_emb_gen import process_single_gpu_gene_pairs\n",
    "from spatial_pattern_classification.spatial_pattern_cls import *\n",
    "from function_prediction.run_gene_level_ours import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861b313",
   "metadata": {},
   "source": [
    "## Initialize args and fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6873f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "args = Namespace(\n",
    "    ckpt_path=\"SpotFormer_checkpoint.ckpt\",\n",
    "    dataset_name=\"cosmx_lung5_rep1\",\n",
    "    target_dataset=\"cosmx_lung5_rep1\",\n",
    "    gene_pct=100,\n",
    "    radius=20,\n",
    "    max_points=20,\n",
    "    cell_gene_pairs_path=\"/path/to/cell_gene_pairs.csv\",\n",
    "    linear_hidden_dim=256,\n",
    "    config=\"configs/bert_config.json\",\n",
    "    seed=42,\n",
    "    f=None,\n",
    "    split_slice=None,\n",
    "    target_width=6724,\n",
    "    target_height=5885,\n",
    "    x_min=None,\n",
    "    x_max=None,\n",
    "    y_min=None,\n",
    "    y_max=None,\n",
    "    sample_ratio=1.0,\n",
    "    merge_threshold=0.5,\n",
    "    percentile=70\n",
    ")\n",
    "\n",
    "print('ckpt_path:', args.ckpt_path)\n",
    "print(f\"Dataset: {args.dataset_name}, radius: {args.radius}, max_points: {args.max_points}\")\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36432216",
   "metadata": {},
   "source": [
    "## Generate cell-specific gene embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = f\"cached_spot_data/spot_input_spot_all_{args.dataset_name}_{args.seed}_{args.gene_pct}_{args.radius}_{args.max_points}.h5\"\n",
    "\n",
    "print(\"Processing cell-gene pairs for gene embeddings -> AnnData\")\n",
    "adata_result = process_single_gpu_gene_pairs(\n",
    "    args, args.cell_gene_pairs_path, args.target_dataset, cache_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de92afc",
   "metadata": {},
   "source": [
    "## Task1: Gene spatial pattern classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_path = 'gene_embeddings_adata/seqfish_gene_embeddings.h5ad'\n",
    "cell_gene_pairs_path = \"Ground_truth/13059_2024_3217_MOESM4_ESM.csv\"\n",
    "\n",
    "\n",
    "results = optimized_clustering_pipeline(\n",
    "    adata_path=adata_path,\n",
    "    cell_gene_pairs_path=cell_gene_pairs_path,\n",
    "    n_clusters_range=range(8, 25, 2),\n",
    "    threshold_range=np.arange(0.3, 0.8, 0.05),\n",
    "    optimize_for='combined_all',  # combine optimization\n",
    "    none_method='hybrid',\n",
    "    target_none_ratio=0.1735,\n",
    "    save_n_clusters=14,\n",
    "    output_dir='seqfish_clustering_results_with_acc'\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline completed successfully!\")\n",
    "\n",
    "if results is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ACCESSING n_clusters=14 RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Extract results for n_clusters=14\n",
    "    n14_results = results['saved_n14_results']\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Available data for n_clusters=14:\")\n",
    "    print(f\"   - Cluster labels: {n14_results['cluster_labels'].shape}\")\n",
    "    print(f\"   - Cluster centers: {n14_results['cluster_centers'].shape}\")\n",
    "    print(f\"   - Cluster label probs: {n14_results['cluster_probs'].shape}\")\n",
    "    print(f\"   - Cluster sizes: {n14_results['cluster_sizes'].shape}\")\n",
    "    print(f\"   - Best threshold: {n14_results['best_threshold']:.3f}\")\n",
    "\n",
    "    cluster_labels_14 = n14_results['cluster_labels']\n",
    "    cluster_centers_14 = n14_results['cluster_centers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8538173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "import pickle\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "from omics.constants import *\n",
    "import os\n",
    "from omics.constants import *\n",
    "import random\n",
    "import h5py\n",
    "from step2_unified_get_gene_emb import process_single_gpu_gene_embeddings\n",
    "from step3_gene_mapping import convert_gene_names_to_ids\n",
    "from step4_merge_emb import merge_embedding_csvs, merge_gene_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3bf085",
   "metadata": {},
   "source": [
    "## Generate universal gene embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123fa30",
   "metadata": {},
   "source": [
    "### Get gene list for function prediction benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e4ce5",
   "metadata": {},
   "source": [
    "#### Peek gene list from benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_genes_from_benchmark_results(result_files):\n",
    "    \"\"\"extract all used genes from benchmark results\"\"\"\n",
    "    all_genes = set()\n",
    "    \n",
    "    for file_path in result_files:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "            \n",
    "            for disease_term, result_df in results.items():\n",
    "                print(f\"  - {disease_term}: {len(result_df)} embedding methods\")\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return all_genes\n",
    "\n",
    "# List of benchmark result files\n",
    "result_files = [\n",
    "    \"../gene-embedding-benchmarks/results/gene_level/go_all_holdout_results.pkl\",\n",
    "    \"../gene-embedding-benchmarks/results/gene_level/go_all_holdout_results_after_2020.pkl\", \n",
    "    \"../gene-embedding-benchmarks/results/gene_level/omim_holdout_results.pkl\"\n",
    "]\n",
    "\n",
    "# Inspect pickle file structure\n",
    "def inspect_pickle_structure(file_path):\n",
    "    \"\"\"Check the structure of pickle file\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(f\"\\n=== {file_path} Structure Analysis ===\")\n",
    "    print(f\"Data type: {type(data)}\")\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        print(f\"Number of dictionary keys: {len(data)}\")\n",
    "        print(\"First few keys:\", list(data.keys())[:3])\n",
    "        \n",
    "        # Check the structure of the first value\n",
    "        first_key = list(data.keys())[0]\n",
    "        first_value = data[first_key]\n",
    "        print(f\"\\nFirst value ({first_key}) type: {type(first_value)}\")\n",
    "        \n",
    "        if isinstance(first_value, pd.DataFrame):\n",
    "            print(f\"DataFrame shape: {first_value.shape}\")\n",
    "            print(f\"Column names: {first_value.columns.tolist()}\")\n",
    "            print(f\"First few indices: {first_value.index[:3].tolist()}\")\n",
    "            print(f\"Example data:\\n{first_value.head()}\")\n",
    "            \n",
    "    return data\n",
    "\n",
    "# Check the structure of each file\n",
    "for file_path in result_files:\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            data = inspect_pickle_structure(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot read {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72ec78",
   "metadata": {},
   "source": [
    "#### Get gene list from benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def read_pickle_with_compatibility(file_path):\n",
    "    \n",
    "    # Method 1: Standard pickle reading\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    except Exception as e1:\n",
    "        print(f\"    Error: Standard pickle reading failed: {str(e1)[:100]}...\")\n",
    "        \n",
    "        # Method 2: Ignore pandas version warnings\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                return data\n",
    "        except Exception as e2:\n",
    "            print(f\"    Error: Ignoring warnings failed: {str(e2)[:100]}...\")\n",
    "            \n",
    "            # Method 3: Use pandas to read directly\n",
    "            try:\n",
    "                data = pd.read_pickle(file_path)\n",
    "                return data\n",
    "            except Exception as e3:\n",
    "                print(f\"    Error: pandas reading failed: {str(e3)[:100]}...\")\n",
    "                \n",
    "                # Method 4: Degrade processing - try to reconstruct DataFrame\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        # Use pickleåº•å±‚åè®®\n",
    "                        import pickle\n",
    "                        data = pickle.load(f)\n",
    "                        \n",
    "                        # If it's a dictionary, try to fix the DataFrame\n",
    "                        if isinstance(data, dict):\n",
    "                            fixed_data = {}\n",
    "                            for key, value in data.items():\n",
    "                                if hasattr(value, 'values') and hasattr(value, 'columns'):\n",
    "                                    # Try to reconstruct DataFrame\n",
    "                                    try:\n",
    "                                        if hasattr(value, 'index'):\n",
    "                                            new_df = pd.DataFrame(\n",
    "                                                data=value.values,\n",
    "                                                columns=value.columns,\n",
    "                                                index=value.index\n",
    "                                            )\n",
    "                                        else:\n",
    "                                            new_df = pd.DataFrame(\n",
    "                                                data=value.values,\n",
    "                                                columns=value.columns\n",
    "                                            )\n",
    "                                        fixed_data[key] = new_df\n",
    "                                    except:\n",
    "                                        # If reconstruction fails, skip this entry\n",
    "                                        continue\n",
    "                                else:\n",
    "                                    fixed_data[key] = value\n",
    "                            return fixed_data\n",
    "                        else:\n",
    "                            return data\n",
    "                            \n",
    "                except Exception as e4:\n",
    "                    print(f\"    Error: Reconstructing reading failed: {str(e4)[:100]}...\")\n",
    "                    return None\n",
    "\n",
    "def extract_genes_from_dataframe(df):\n",
    "    \"\"\"Extract genes from DataFrame safely\"\"\"\n",
    "    try:\n",
    "        if isinstance(df, pd.DataFrame) and 'gene' in df.columns:\n",
    "            # Ensure gene column is a string type\n",
    "            genes = df['gene'].astype(str).tolist()\n",
    "            # Filter out empty values and NaN\n",
    "            genes = [g for g in genes if g and g != 'nan' and g != 'None']\n",
    "            return set(genes)\n",
    "        else:\n",
    "            return set()\n",
    "    except Exception as e:\n",
    "        print(f\"    Error: Extracting genes: {e}\")\n",
    "        return set()\n",
    "\n",
    "def extract_genes_from_fold_data():\n",
    "    \"\"\"Extract gene list from fold data\"\"\"\n",
    "    \n",
    "    # All possible file locations\n",
    "    possible_files = [\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold1_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold2_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold3_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_holdout_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO_after_2020/go_cv_fold1_dict_after_2020.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO_after_2020/go_cv_fold2_dict_after_2020.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO_after_2020/go_cv_fold3_dict_after_2020.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO_after_2020/go_holdout_dict_after_2020.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold1_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold2_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold3_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_holdout_dict.pkl\"\n",
    "    ]\n",
    "    \n",
    "    all_genes = set()\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in possible_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"\\n=== {file_path} ===\")\n",
    "            \n",
    "            # Use compatibility reading\n",
    "            data = read_pickle_with_compatibility(file_path)\n",
    "            \n",
    "            if data is not None:\n",
    "                print(f\"Data type: {type(data)}\")\n",
    "                \n",
    "                if isinstance(data, dict):\n",
    "                    print(f\"Number of diseases/processes: {len(data)}\")\n",
    "                    \n",
    "                    # Check the structure of the first entry\n",
    "                    if len(data) > 0:\n",
    "                        first_key = list(data.keys())[0]\n",
    "                        first_value = data[first_key]\n",
    "                        print(f\"First entry ({first_key}) type: {type(first_value)}\")\n",
    "                        \n",
    "                        if isinstance(first_value, pd.DataFrame):\n",
    "                            print(f\"DataFrame columns: {first_value.columns.tolist()}\")\n",
    "                            print(f\"DataFrame shape: {first_value.shape}\")\n",
    "                            if 'gene' in first_value.columns:\n",
    "                                print(f\"Contains gene column! First few genes: {first_value['gene'].head().tolist()}\")\n",
    "                    \n",
    "                    # Extract all genes\n",
    "                    file_genes = set()\n",
    "                    successful_terms = 0\n",
    "                    \n",
    "                    for term, df in data.items():\n",
    "                        genes_in_term = extract_genes_from_dataframe(df)\n",
    "                        if genes_in_term:\n",
    "                            file_genes.update(genes_in_term)\n",
    "                            successful_terms += 1\n",
    "                    \n",
    "                    if file_genes:\n",
    "                        print(f\"Extracted {len(file_genes)} genes from this file (from {successful_terms} entries)\")\n",
    "                        all_genes.update(file_genes)\n",
    "                        successful_files.append(file_path)\n",
    "                    else:\n",
    "                        print(\"No gene information found in this file\")\n",
    "                        failed_files.append((file_path, \"No gene information\"))\n",
    "                else:\n",
    "                    print(f\"Data is not a dictionary format: {type(data)}\")\n",
    "                    failed_files.append((file_path, \"Data format error\"))\n",
    "            else:\n",
    "                print(\"All reading methods failed\")\n",
    "                failed_files.append((file_path, \"Reading failed\"))\n",
    "                \n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            failed_files.append((file_path, \"File not found\"))\n",
    "    \n",
    "    return all_genes, successful_files, failed_files\n",
    "\n",
    "# Extract genes from fold data\n",
    "print(\"Extracting genes from fold data...\")\n",
    "benchmark_genes, successful_files, failed_files = extract_genes_from_fold_data()\n",
    "\n",
    "if benchmark_genes:\n",
    "    print(f\"\\n=== Summary of extraction results ===\")\n",
    "    print(f\"Total unique genes found: {len(benchmark_genes)}\")\n",
    "    print(f\"Successful processed files: {len(successful_files)}\")\n",
    "    print(f\"Failed files: {len(failed_files)}\")\n",
    "    \n",
    "    # Show some example genes\n",
    "    sample_genes = sorted(list(benchmark_genes))[:10]\n",
    "    print(f\"Example genes: {sample_genes}\")\n",
    "    \n",
    "    # Save gene list\n",
    "    with open('benchmark_gene_list_complete.txt', 'w') as f:\n",
    "        for gene in sorted(benchmark_genes):\n",
    "            f.write(f\"{gene}\\n\")\n",
    "    \n",
    "    print(\"\\nGene list saved to benchmark_gene_list_complete.txt\")\n",
    "    \n",
    "    # Count by file type\n",
    "    print(f\"\\n=== Successfully extracted sources ===\")\n",
    "    go_files = 0\n",
    "    omim_files = 0\n",
    "    for file_path in successful_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"  âœ“ {filename}\")\n",
    "        if 'omim' in filename.lower():\n",
    "            omim_files += 1\n",
    "        else:\n",
    "            go_files += 1\n",
    "    \n",
    "    print(f\"\\nGO files: {go_files}, OMIM files: {omim_files}\")\n",
    "    \n",
    "    # Show failed files\n",
    "    if failed_files:\n",
    "        print(f\"\\n=== Failed files ===\")\n",
    "        for file_path, reason in failed_files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"  âœ— {filename}: {reason}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot extract gene information, please check other file locations\")\n",
    "    \n",
    "    # Show all failed reasons\n",
    "    print(\"\\nFailed details:\")\n",
    "    for file_path, reason in failed_files:\n",
    "        print(f\"  {os.path.basename(file_path)}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd633a09",
   "metadata": {},
   "source": [
    "#### Convert gene ID into gene name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d86ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygene\n",
    "import pandas as pd\n",
    "\n",
    "def convert_entrez_to_symbols(entrez_file):\n",
    "    \"\"\"Convert Entrez ID to gene symbols\"\"\"\n",
    "    \n",
    "    # Read gene list\n",
    "    with open(entrez_file, 'r') as f:\n",
    "        entrez_ids = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Preparing to convert {len(entrez_ids)} Entrez IDs\")\n",
    "    \n",
    "    # Initialize mygene\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    \n",
    "    # Batch conversion (avoid API limit)\n",
    "    batch_size = 1000\n",
    "    gene_mapping = {}\n",
    "    failed_ids = []\n",
    "    \n",
    "    for i in range(0, len(entrez_ids), batch_size):\n",
    "        batch = entrez_ids[i:i+batch_size]\n",
    "        batch_num = i//batch_size + 1\n",
    "        total_batches = (len(entrez_ids)-1)//batch_size + 1\n",
    "        \n",
    "        print(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} genes)\")\n",
    "        \n",
    "        try:\n",
    "            results = mg.querymany(\n",
    "                batch, \n",
    "                scopes='entrezgene', \n",
    "                fields='symbol,entrezgene,name', \n",
    "                species='human',\n",
    "                returnall=True\n",
    "            )\n",
    "            \n",
    "            # Process successful results\n",
    "            for result in results['out']:\n",
    "                if 'symbol' in result and 'entrezgene' in result:\n",
    "                    entrez_id = str(result['entrezgene'])\n",
    "                    symbol = result['symbol']\n",
    "                    gene_name = result.get('name', '')\n",
    "                    gene_mapping[entrez_id] = {\n",
    "                        'symbol': symbol,\n",
    "                        'name': gene_name\n",
    "                    }\n",
    "                elif 'query' in result:\n",
    "                    # Record failed IDs\n",
    "                    failed_ids.append(result['query'])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Batch {batch_num} conversion failed: {e}\")\n",
    "            failed_ids.extend(batch)\n",
    "    \n",
    "    print(f\"\\nConversion completed!\")\n",
    "    print(f\"Successfully converted: {len(gene_mapping)} genes\")\n",
    "    print(f\"Conversion failed: {len(failed_ids)} genes\")\n",
    "    print(f\"Conversion success rate: {len(gene_mapping)/len(entrez_ids)*100:.1f}%\")\n",
    "    \n",
    "    return gene_mapping, failed_ids\n",
    "\n",
    "print(\"Starting gene ID conversion...\")\n",
    "gene_mapping, failed_ids = convert_entrez_to_symbols('benchmark_gene_list_complete.txt')\n",
    "\n",
    "if gene_mapping:\n",
    "    mapping_df = pd.DataFrame([\n",
    "        {\n",
    "            'entrez_id': entrez_id,\n",
    "            'gene_symbol': info['symbol'], \n",
    "            'gene_name': info['name']\n",
    "        }\n",
    "        for entrez_id, info in gene_mapping.items()\n",
    "    ])\n",
    "    \n",
    "    mapping_df.to_csv('benchmark_gene_mapping.csv', index=False)\n",
    "    print(f\"Detailed mapping table saved to benchmark_gene_mapping.csv\")\n",
    "    \n",
    "    symbols = sorted([info['symbol'] for info in gene_mapping.values()])\n",
    "    with open('benchmark_gene_symbols.txt', 'w') as f:\n",
    "        for symbol in symbols:\n",
    "            f.write(f\"{symbol}\\n\")\n",
    "    \n",
    "    print(f\"\\nExample conversion results:\")\n",
    "    for i, (entrez_id, info) in enumerate(list(gene_mapping.items())[:10]):\n",
    "        print(f\"  {entrez_id} -> {info['symbol']} ({info['name'][:50]}...)\")\n",
    "    \n",
    "    if failed_ids:\n",
    "        with open('failed_entrez_ids.txt', 'w') as f:\n",
    "            for failed_id in failed_ids:\n",
    "                f.write(f\"{failed_id}\\n\")\n",
    "        print(f\"\\n{len(failed_ids)} failed IDs saved to failed_entrez_ids.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4657d5f",
   "metadata": {},
   "source": [
    "#### map into our dataset's gene list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "merfish_adata = sc.read_h5ad('merfish_processed.h5ad')\n",
    "merfish_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca359f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmx_data = pd.read_pickle('CosMx/spot_dataframe.pkl')\n",
    "CosMx_gene = np.unique(cosmx_data['gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13408605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_compare_genes():\n",
    "    \"\"\"Extract genes from actual data and compare with benchmark\"\"\"\n",
    "    \n",
    "    # Read benchmark test genes\n",
    "    with open('benchmark_gene_symbols.txt', 'r') as f:\n",
    "        benchmark_genes = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "    print(f\"Total benchmark test genes: {len(benchmark_genes)}\")\n",
    "    \n",
    "    # === Method 1: If you have merfish_adata object ===\n",
    "    try:\n",
    "        # Assuming your merfish_adata is already loaded\n",
    "        merfish_genes = list(merfish_adata.var_names)\n",
    "        # Filter out non-target genes\n",
    "        merfish_genes = [g for g in merfish_genes if not g.startswith('notarget')]\n",
    "        print(f\"MERFISH genes (filtered): {len(merfish_genes)}\")\n",
    "    except:\n",
    "        print(\"Please load merfish_adata first\")\n",
    "        merfish_genes = []\n",
    "    \n",
    "    # === Method 2: If you have CosMx data file ===\n",
    "    try:\n",
    "        # Read from pickle file\n",
    "        cosmx_data = pd.read_pickle('CosMx/spot_dataframe.pkl')\n",
    "        cosmx_genes = list(np.unique(cosmx_data['gene']))\n",
    "        print(f\"CosMx genes: {len(cosmx_genes)}\")\n",
    "    except:\n",
    "        print(\"Cannot read CosMx data file\")\n",
    "        cosmx_genes = []\n",
    "    \n",
    "    # Perform matching analysis\n",
    "    if merfish_genes or cosmx_genes:\n",
    "        # Convert to uppercase for matching\n",
    "        benchmark_upper = set(g.upper() for g in benchmark_genes)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if merfish_genes:\n",
    "            merfish_upper = set(g.upper() for g in merfish_genes)\n",
    "            merfish_overlap = merfish_upper.intersection(benchmark_upper)\n",
    "            \n",
    "            print(f\"\\n=== MERFISH matching results ===\")\n",
    "            print(f\"MERFISH genes: {len(merfish_genes)}\")\n",
    "            print(f\"Overlap with benchmark: {len(merfish_overlap)}\")\n",
    "            print(f\"Overlap rate: {len(merfish_overlap)/len(benchmark_upper)*100:.1f}%\")\n",
    "            \n",
    "            sample_overlap = sorted(list(merfish_overlap))[:10]\n",
    "            print(f\"Overlapping genes example: {sample_overlap}\")\n",
    "            \n",
    "            results['merfish'] = {\n",
    "                'total': len(merfish_genes),\n",
    "                'overlap': len(merfish_overlap),\n",
    "                'overlap_genes': sorted(merfish_overlap)\n",
    "            }\n",
    "        \n",
    "        if cosmx_genes:\n",
    "            cosmx_upper = set(g.upper() for g in cosmx_genes)\n",
    "            cosmx_overlap = cosmx_upper.intersection(benchmark_upper)\n",
    "            \n",
    "            # Show some overlapping genes\n",
    "            sample_overlap = sorted(list(cosmx_overlap))[:10]\n",
    "            print(f\"Overlapping genes example: {sample_overlap}\")\n",
    "            \n",
    "            results['cosmx'] = {\n",
    "                'total': len(cosmx_genes),\n",
    "                'overlap': len(cosmx_overlap),\n",
    "                'overlap_genes': sorted(cosmx_overlap)\n",
    "            }\n",
    "        \n",
    "        # If both datasets exist, compare them\n",
    "        if merfish_genes and cosmx_genes:\n",
    "            merfish_upper = set(g.upper() for g in merfish_genes)\n",
    "            cosmx_upper = set(g.upper() for g in cosmx_genes)\n",
    "            \n",
    "            both_overlap = merfish_upper.intersection(cosmx_upper)\n",
    "            three_way_overlap = merfish_overlap.intersection(cosmx_overlap)\n",
    "            \n",
    "            print(f\"\\n=== Comparison between datasets ===\")\n",
    "            print(f\"MERFISH and CosMx overlap: {len(both_overlap)}\")\n",
    "            print(f\"Three-way overlap (all match benchmark): {len(three_way_overlap)}\")\n",
    "            \n",
    "            results['comparison'] = {\n",
    "                'merfish_cosmx_overlap': len(both_overlap),\n",
    "                'three_way_overlap': len(three_way_overlap),\n",
    "                'three_way_genes': sorted(three_way_overlap)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    else:\n",
    "        print(\"No available gene data for comparison\")\n",
    "        return None\n",
    "\n",
    "results = extract_and_compare_genes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1649e",
   "metadata": {},
   "source": [
    "#### Save matched genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_matched_genes_for_embedding():\n",
    "    \n",
    "    with open('benchmark_gene_symbols.txt', 'r') as f:\n",
    "        benchmark_genes = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "    try:\n",
    "        # MERFISH\n",
    "        merfish_genes = list(merfish_adata.var_names)\n",
    "        merfish_genes_filtered = [g for g in merfish_genes if not g.startswith('notarget')]\n",
    "        \n",
    "        # CosMx\n",
    "        cosmx_data = pd.read_pickle('CosMx/spot_dataframe.pkl')\n",
    "        cosmx_genes = list(np.unique(cosmx_data['gene']))\n",
    "        \n",
    "        # Calculate overlap\n",
    "        benchmark_upper = set(g.upper() for g in benchmark_genes)\n",
    "        merfish_upper = set(g.upper() for g in merfish_genes_filtered)\n",
    "        cosmx_upper = set(g.upper() for g in cosmx_genes)\n",
    "        \n",
    "        merfish_overlap = merfish_upper.intersection(benchmark_upper)\n",
    "        cosmx_overlap = cosmx_upper.intersection(benchmark_upper)\n",
    "        \n",
    "        # Save overlapping genes\n",
    "        with open('merfish_benchmark_genes.txt', 'w') as f:\n",
    "            for gene in sorted(merfish_overlap):\n",
    "                f.write(f\"{gene}\\n\")\n",
    "        \n",
    "        with open('cosmx_benchmark_genes.txt', 'w') as f:\n",
    "            for gene in sorted(cosmx_overlap):\n",
    "                f.write(f\"{gene}\\n\")\n",
    "        \n",
    "        # Save combined gene list (for complete benchmark)\n",
    "        combined_genes = merfish_overlap.union(cosmx_overlap)\n",
    "        with open('combined_benchmark_genes.txt', 'w') as f:\n",
    "            for gene in sorted(combined_genes):\n",
    "                f.write(f\"{gene}\\n\")\n",
    "        \n",
    "        # Find three-way overlap (for direct comparison)\n",
    "        three_way_overlap = merfish_overlap.intersection(cosmx_overlap)\n",
    "        with open('three_way_overlap_genes.txt', 'w') as f:\n",
    "            for gene in sorted(three_way_overlap):\n",
    "                f.write(f\"{gene}\\n\")\n",
    "\n",
    "        return {\n",
    "            'merfish_genes': sorted(merfish_overlap),\n",
    "            'cosmx_genes': sorted(cosmx_overlap),\n",
    "            'combined_genes': sorted(combined_genes),\n",
    "            'three_way_genes': sorted(three_way_overlap)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing gene data: {e}\")\n",
    "        return None\n",
    "\n",
    "gene_lists = save_matched_genes_for_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfc032",
   "metadata": {},
   "source": [
    "#### filter new gene list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b57f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_subsets_with_compatibility():\n",
    "    \n",
    "    import pickle\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    with open('combined_benchmark_genes.txt', 'r') as f:\n",
    "        target_genes = set(line.strip().upper() for line in f)\n",
    "    \n",
    "    print(f\"Target gene number: {len(target_genes)}\")\n",
    "    \n",
    "    gene_mapping = pd.read_csv('benchmark_gene_mapping.csv')\n",
    "    entrez_to_symbol = dict(zip(gene_mapping['entrez_id'].astype(str), \n",
    "                               gene_mapping['gene_symbol'].str.upper()))\n",
    "    \n",
    "    benchmark_files = [\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold1_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold2_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold3_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_holdout_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold1_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold2_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold3_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_holdout_dict.pkl\"\n",
    "    ]\n",
    "    \n",
    "    os.makedirs('test_subsets', exist_ok=True)\n",
    "    \n",
    "    subset_stats = []\n",
    "    \n",
    "    for file_path in benchmark_files:\n",
    "        if os.path.exists(file_path):\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"\\nProcessing {filename}...\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    original_data = pickle.load(f)\n",
    "                \n",
    "            except Exception as e1:\n",
    "                print(f\"Error: {e1}\")\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        original_data = pickle.load(f)\n",
    "                        \n",
    "                except Exception as e2:\n",
    "                    print(f\"Protocol=2 read failed: {e2}\")\n",
    "                    \n",
    "                    try:\n",
    "                        original_data = pd.read_pickle(file_path)\n",
    "                        \n",
    "                    except Exception as e3:\n",
    "                        print(f\"pd.read_pickle read failed: {e3}\")\n",
    "                        continue\n",
    "            \n",
    "            # If successfully read, process\n",
    "            try:\n",
    "                subset_data = {}\n",
    "                total_samples = 0\n",
    "                kept_samples = 0\n",
    "                \n",
    "                for term, data in original_data.items():\n",
    "                    # Process different data formats\n",
    "                    if isinstance(data, pd.DataFrame):\n",
    "                        df = data\n",
    "                    elif hasattr(data, 'to_frame'):\n",
    "                        df = data.to_frame()\n",
    "                    elif isinstance(data, dict):\n",
    "                        df = pd.DataFrame(data)\n",
    "                    else:\n",
    "                        # Try to convert to DataFrame\n",
    "                        try:\n",
    "                            df = pd.DataFrame(data)\n",
    "                        except:\n",
    "                            print(f\"Cannot process term {term} data format\")\n",
    "                            continue\n",
    "                    \n",
    "                    if 'gene' in df.columns:\n",
    "                        total_samples += len(df)\n",
    "                        \n",
    "                        # Create new DataFrame to avoid version issues\n",
    "                        new_df = pd.DataFrame({\n",
    "                            'gene': df['gene'].values,\n",
    "                            'result': df['result'].values if 'result' in df.columns else [1] * len(df)\n",
    "                        })\n",
    "                        \n",
    "                        # Convert gene ID\n",
    "                        new_df['gene_symbol'] = new_df['gene'].astype(str).map(entrez_to_symbol)\n",
    "                        \n",
    "                        # Filter target genes\n",
    "                        mask = new_df['gene_symbol'].isin(target_genes)\n",
    "                        subset_df = new_df[mask][['gene', 'result']].copy()\n",
    "                        \n",
    "                        if len(subset_df) > 0:\n",
    "                            subset_data[term] = subset_df\n",
    "                            kept_samples += len(subset_df)\n",
    "                \n",
    "                # Save subset file\n",
    "                if subset_data:\n",
    "                    output_filename = filename.replace('.pkl', '_subset.pkl')\n",
    "                    output_path = os.path.join('test_subsets', output_filename)\n",
    "                    \n",
    "                    with open(output_path, 'wb') as f:\n",
    "                        pickle.dump(subset_data, f, protocol=2)  # Use better compatible protocol\n",
    "                    \n",
    "                    subset_stats.append({\n",
    "                        'file': output_filename,\n",
    "                        'terms': len(subset_data),\n",
    "                        'original_samples': total_samples,\n",
    "                        'kept_samples': kept_samples,\n",
    "                        'retention_rate': f\"{kept_samples/total_samples*100:.1f}%\" if total_samples > 0 else \"0%\"\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"{output_filename}: {len(subset_data)} terms, {kept_samples}/{total_samples} samples\")\n",
    "                else:\n",
    "                    print(f\"{filename}: no matching data\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"{filename}: error processing data: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== Summary of subset creation ===\")\n",
    "    print(f\"Successfully created {len(subset_stats)} test subset files\")\n",
    "    \n",
    "    for stat in subset_stats:\n",
    "        print(f\"  {stat['file']}: {stat['terms']} terms, {stat['retention_rate']} retention\")\n",
    "    \n",
    "    return subset_stats\n",
    "\n",
    "subset_stats = prepare_test_subsets_with_compatibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe26fda",
   "metadata": {},
   "source": [
    "### Generate gene embeddings for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9390f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    ckpt_path=\"SpotFormer_checkpoint.ckpt\",\n",
    "    dataset_name=\"cosmx_lung5_rep1\",\n",
    "    target_dataset=\"cosmx_lung5_rep1\",\n",
    "    gene_pct=100,\n",
    "    linear_hidden_dim='256',\n",
    "    radius=20,\n",
    "    max_points=20,\n",
    "    config='configs/bert_config.json',\n",
    "    seed=42,\n",
    "    f=None,\n",
    "    split_slice=None,\n",
    "    target_width=6724,\n",
    "    target_height=5885,\n",
    "    x_min=None,\n",
    "    x_max=None,\n",
    "    y_min=None,\n",
    "    y_max=None,\n",
    "    sample_ratio=1.0,\n",
    "    merge_threshold=0.5,\n",
    "    percentile=70,\n",
    "    gene_list_path=\"cosmx_benchmark_genes.txt\"\n",
    ")\n",
    "\n",
    "args.linear_hidden_dim = '256'\n",
    "args.radius = 20\n",
    "args.max_points = 20\n",
    "args.config = 'configs/bert_config.json'\n",
    "print(f\"Dataset: {args.dataset_name}, radius: {args.radius}, max_points: {args.max_points}\")\n",
    "\n",
    "if args.dataset_name == 'cosmx_lung5_rep1':\n",
    "    cache_path = f'spot_input_spot_sampled_{args.dataset_name}_{args.seed}_{args.gene_pct}_{args.radius}_{args.max_points}_0.1.h5'\n",
    "else:\n",
    "    cache_path = f'spot_input_spot_all_{args.dataset_name}_{args.seed}_{args.gene_pct}_{args.radius}_{args.max_points}.h5'\n",
    "\n",
    "print(\"Processing gene-level embeddings from gene list -> CSV format\")\n",
    "emb_file, genelist_file = process_single_gpu_gene_embeddings(\n",
    "    args, args.gene_list_path, args.target_dataset, cache_path\n",
    ")\n",
    "\n",
    "# Construct cache path\n",
    "if args.dataset_name == 'cosmx_lung5_rep1':\n",
    "    cache_path = f'spot_input_spot_sampled_{args.dataset_name}_{args.seed}_{args.gene_pct}_{args.radius}_{args.max_points}_0.1.h5'\n",
    "else:\n",
    "    cache_path = f'spot_input_spot_all_{args.dataset_name}_{args.seed}_{args.gene_pct}_{args.radius}_{args.max_points}.h5'\n",
    "\n",
    "print(\"Processing gene-level embeddings from gene list -> CSV format\")\n",
    "emb_file, genelist_file = process_single_gpu_gene_embeddings(\n",
    "    args, args.gene_list_path, args.target_dataset, cache_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590cf2d",
   "metadata": {},
   "source": [
    "### Convert gene name list to gene ID list using benchmark gene mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    genelist_file='merfish_gene100pct_Spotformer_genelist.txt',\n",
    "    mapping_file=\"benchmark_gene_mapping.csv\",\n",
    "    output_file='Spotformer_genelist2.txt'\n",
    ")\n",
    "\n",
    "if args.output_file is None:\n",
    "    base_name = os.path.splitext(args.genelist_file)[0]\n",
    "    if base_name.endswith('_genelist'):\n",
    "        base_name = base_name[:-9]\n",
    "    args.output_file = f\"{base_name}_geneid.txt\"\n",
    "\n",
    "print(f\"Converting gene names to IDs...\")\n",
    "print(f\"  Input: {args.genelist_file}\")\n",
    "print(f\"  Mapping: {args.mapping_file}\")\n",
    "print(f\"  Output: {args.output_file}\")\n",
    "\n",
    "gene_ids, matched_count, unmatched_genes = convert_gene_names_to_ids(\n",
    "    args.genelist_file, args.mapping_file, args.output_file\n",
    ")\n",
    "\n",
    "print(f\"\\nDone! Generated {len(gene_ids)} gene IDs with {matched_count} matches.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7de9c",
   "metadata": {},
   "source": [
    "### Merge gene embeddings from different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e1cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    emb_file1=\"cosmx_lung5_rep1_gene20pct_Spotformer_emb.csv\",\n",
    "    emb_file2=\"merfish_gene100pct_Spotformer_emb.csv\",\n",
    "    gene_file1=\"Spotformer_genelist.txt\",\n",
    "    gene_file2=\"Spotformer_genelist2.txt\",\n",
    "    output_dir=\"gene_embeddings_csv/\",\n",
    "    output_prefix=\"merged\"\n",
    ")\n",
    "\n",
    "merged_emb_file = os.path.join(args.output_dir, f\"{args.output_prefix}_Spotformer_emb.csv\")\n",
    "merged_gene_file = os.path.join(args.output_dir, f\"{args.output_prefix}_Spotformer_genelist.txt\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MERGING EMBEDDING FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Merge embedding files\n",
    "merged_embeddings = merge_embedding_csvs(\n",
    "    args.emb_file1, args.emb_file2, merged_emb_file\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MERGING GENE LIST FILES\") \n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Merge gene list files\n",
    "merged_genes = merge_gene_lists(\n",
    "    args.gene_file1, args.gene_file2, merged_gene_file\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify consistency\n",
    "if len(merged_genes) == len(merged_embeddings):\n",
    "    print(f\"âœ“ Consistency check passed: {len(merged_genes)} genes match {len(merged_embeddings)} embeddings\")\n",
    "else:\n",
    "    print(f\"âœ— Consistency check failed: {len(merged_genes)} genes vs {len(merged_embeddings)} embeddings\")\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  Merged embeddings: {merged_emb_file}\")\n",
    "print(f\"  Merged gene list: {merged_gene_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5f41f",
   "metadata": {},
   "source": [
    "## Task2: Gene function prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 1. Extract embeddings\n",
    "    (\n",
    "        reindexed_embeddings,\n",
    "        gene_lists,\n",
    "        reference_node2index,\n",
    "        reference_genes,\n",
    "    ) = helper.get_embeddings('embeddings/intersect/ours')\n",
    "    \n",
    "    print(f\"Successfully loaded embeddings:\")\n",
    "    for subfolder, embedding_df in reindexed_embeddings.items():\n",
    "        print(f\"  {os.path.basename(subfolder)}: shape {embedding_df.shape}\")\n",
    "    print(f\"Number of genes: {len(reference_genes)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load embeddings: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    # 2. Load test subsets\n",
    "    go_data = load_test_subsets()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load test subsets: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 3. GO benchmark\n",
    "if go_data:\n",
    "    try:\n",
    "        print(\"\\n=== Running GO safe benchmark ===\")\n",
    "        \n",
    "        go_combined = combine_all_folds(\n",
    "            go_data['cv_fold1'],\n",
    "            go_data['cv_fold2'],\n",
    "            go_data['cv_fold3'],\n",
    "            go_data['holdout']\n",
    "        )\n",
    "        \n",
    "        print(f\"GO combined data: {len(go_combined)} terms\")\n",
    "        \n",
    "        go_results, go_stats = run_safe_benchmark(reindexed_embeddings, go_combined)\n",
    "        \n",
    "        # Safe results\n",
    "        os.makedirs(\"results_subset\", exist_ok=True)\n",
    "        with open(\"results_subset/go_safe_results.pkl\", \"wb\") as f:\n",
    "            pickle.dump(go_results, f)\n",
    "        \n",
    "        with open(\"results_subset/go_stats.pkl\", \"wb\") as f:\n",
    "            pickle.dump(go_stats, f)\n",
    "        \n",
    "        # Summarize results\n",
    "        go_performance = summarize_results(go_results, \"GO\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to run GO benchmark: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
