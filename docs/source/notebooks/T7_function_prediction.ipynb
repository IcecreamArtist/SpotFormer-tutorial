{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d230a9",
   "metadata": {},
   "source": [
    "# Tutorial 5: gene function prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdae5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "import pickle\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "from omics.constants import *\n",
    "import os\n",
    "from omics.constants import *\n",
    "import random\n",
    "import h5py\n",
    "from step2_unified_get_gene_emb import process_single_gpu_gene_embeddings\n",
    "from step3_gene_mapping import convert_gene_names_to_ids\n",
    "from step4_merge_emb import merge_embedding_csvs, merge_gene_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2af08",
   "metadata": {},
   "source": [
    "## Get gene list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50604300",
   "metadata": {},
   "source": [
    "### Peek gene list from benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_genes_from_benchmark_results(result_files):\n",
    "    \"\"\"从基准测试结果中提取所有使用过的基因\"\"\"\n",
    "    all_genes = set()\n",
    "    \n",
    "    for file_path in result_files:\n",
    "        print(f\"处理文件: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "            \n",
    "            # 遍历每个疾病/生物过程的结果\n",
    "            for disease_term, result_df in results.items():\n",
    "                print(f\"  - {disease_term}: {len(result_df)} 个嵌入方法\")\n",
    "                \n",
    "                # 这里的result_df应该包含不同嵌入方法的结果\n",
    "                # 但基因信息可能在更深的结构中\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"读取 {file_path} 时出错: {e}\")\n",
    "    \n",
    "    return all_genes\n",
    "\n",
    "# 基准测试结果文件列表\n",
    "result_files = [\n",
    "    \"../gene-embedding-benchmarks/results/gene_level/go_all_holdout_results.pkl\",\n",
    "    \"../gene-embedding-benchmarks/results/gene_level/go_all_holdout_results_after_2020.pkl\", \n",
    "    \"../gene-embedding-benchmarks/results/gene_level/omim_holdout_results.pkl\"\n",
    "]\n",
    "\n",
    "# 先看看文件结构\n",
    "def inspect_pickle_structure(file_path):\n",
    "    \"\"\"检查pickle文件的结构\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(f\"\\n=== {file_path} 结构分析 ===\")\n",
    "    print(f\"数据类型: {type(data)}\")\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        print(f\"字典键的数量: {len(data)}\")\n",
    "        print(\"前几个键:\", list(data.keys())[:3])\n",
    "        \n",
    "        # 检查第一个值的结构\n",
    "        first_key = list(data.keys())[0]\n",
    "        first_value = data[first_key]\n",
    "        print(f\"\\n第一个值 ({first_key}) 的类型: {type(first_value)}\")\n",
    "        \n",
    "        if isinstance(first_value, pd.DataFrame):\n",
    "            print(f\"DataFrame形状: {first_value.shape}\")\n",
    "            print(f\"列名: {first_value.columns.tolist()}\")\n",
    "            print(f\"索引前几个: {first_value.index[:3].tolist()}\")\n",
    "            print(f\"示例数据:\\n{first_value.head()}\")\n",
    "            \n",
    "    return data\n",
    "\n",
    "# 检查每个文件的结构\n",
    "for file_path in result_files:\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            data = inspect_pickle_structure(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"无法读取 {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"文件不存在: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b7fa4",
   "metadata": {},
   "source": [
    "### Get gene list from benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def read_pickle_with_compatibility(file_path):\n",
    "    \"\"\"兼容性pickle读取函数\"\"\"\n",
    "    \n",
    "    # 方法1: 标准pickle读取\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    except Exception as e1:\n",
    "        print(f\"    标准读取失败: {str(e1)[:100]}...\")\n",
    "        \n",
    "        # 方法2: 忽略pandas版本警告\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                return data\n",
    "        except Exception as e2:\n",
    "            print(f\"    忽略警告读取失败: {str(e2)[:100]}...\")\n",
    "            \n",
    "            # 方法3: 使用pandas直接读取\n",
    "            try:\n",
    "                data = pd.read_pickle(file_path)\n",
    "                return data\n",
    "            except Exception as e3:\n",
    "                print(f\"    pandas读取失败: {str(e3)[:100]}...\")\n",
    "                \n",
    "                # 方法4: 降级处理 - 尝试重构DataFrame\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        # 使用pickle底层协议\n",
    "                        import pickle\n",
    "                        data = pickle.load(f)\n",
    "                        \n",
    "                        # 如果是字典，尝试修复其中的DataFrame\n",
    "                        if isinstance(data, dict):\n",
    "                            fixed_data = {}\n",
    "                            for key, value in data.items():\n",
    "                                if hasattr(value, 'values') and hasattr(value, 'columns'):\n",
    "                                    # 尝试重建DataFrame\n",
    "                                    try:\n",
    "                                        if hasattr(value, 'index'):\n",
    "                                            new_df = pd.DataFrame(\n",
    "                                                data=value.values,\n",
    "                                                columns=value.columns,\n",
    "                                                index=value.index\n",
    "                                            )\n",
    "                                        else:\n",
    "                                            new_df = pd.DataFrame(\n",
    "                                                data=value.values,\n",
    "                                                columns=value.columns\n",
    "                                            )\n",
    "                                        fixed_data[key] = new_df\n",
    "                                    except:\n",
    "                                        # 如果重建失败，跳过这个条目\n",
    "                                        continue\n",
    "                                else:\n",
    "                                    fixed_data[key] = value\n",
    "                            return fixed_data\n",
    "                        else:\n",
    "                            return data\n",
    "                            \n",
    "                except Exception as e4:\n",
    "                    print(f\"    重构读取失败: {str(e4)[:100]}...\")\n",
    "                    return None\n",
    "\n",
    "def extract_genes_from_dataframe(df):\n",
    "    \"\"\"从DataFrame中安全提取基因\"\"\"\n",
    "    try:\n",
    "        if isinstance(df, pd.DataFrame) and 'gene' in df.columns:\n",
    "            # 确保基因列是字符串类型\n",
    "            genes = df['gene'].astype(str).tolist()\n",
    "            # 过滤掉空值和NaN\n",
    "            genes = [g for g in genes if g and g != 'nan' and g != 'None']\n",
    "            return set(genes)\n",
    "        else:\n",
    "            return set()\n",
    "    except Exception as e:\n",
    "        print(f\"      提取基因时出错: {e}\")\n",
    "        return set()\n",
    "\n",
    "def extract_genes_from_fold_data():\n",
    "    \"\"\"从fold数据中提取基因列表\"\"\"\n",
    "    \n",
    "    # 所有可能的文件位置\n",
    "    possible_files = [\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold1_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold2_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold3_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_holdout_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO_after_2020/go_cv_fold1_dict_after_2020.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO_after_2020/go_cv_fold2_dict_after_2020.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO_after_2020/go_cv_fold3_dict_after_2020.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO_after_2020/go_holdout_dict_after_2020.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold1_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold2_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold3_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_holdout_dict.pkl\"\n",
    "    ]\n",
    "    \n",
    "    all_genes = set()\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_path in possible_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"\\n=== {file_path} ===\")\n",
    "            \n",
    "            # 使用兼容性读取\n",
    "            data = read_pickle_with_compatibility(file_path)\n",
    "            \n",
    "            if data is not None:\n",
    "                print(f\"数据类型: {type(data)}\")\n",
    "                \n",
    "                if isinstance(data, dict):\n",
    "                    print(f\"疾病/过程数量: {len(data)}\")\n",
    "                    \n",
    "                    # 检查第一个条目的结构\n",
    "                    if len(data) > 0:\n",
    "                        first_key = list(data.keys())[0]\n",
    "                        first_value = data[first_key]\n",
    "                        print(f\"第一个条目 ({first_key}) 类型: {type(first_value)}\")\n",
    "                        \n",
    "                        if isinstance(first_value, pd.DataFrame):\n",
    "                            print(f\"DataFrame列: {first_value.columns.tolist()}\")\n",
    "                            print(f\"DataFrame形状: {first_value.shape}\")\n",
    "                            if 'gene' in first_value.columns:\n",
    "                                print(f\"包含基因列！前几个基因: {first_value['gene'].head().tolist()}\")\n",
    "                    \n",
    "                    # 提取所有基因\n",
    "                    file_genes = set()\n",
    "                    successful_terms = 0\n",
    "                    \n",
    "                    for term, df in data.items():\n",
    "                        genes_in_term = extract_genes_from_dataframe(df)\n",
    "                        if genes_in_term:\n",
    "                            file_genes.update(genes_in_term)\n",
    "                            successful_terms += 1\n",
    "                    \n",
    "                    if file_genes:\n",
    "                        print(f\"从该文件提取到 {len(file_genes)} 个基因 (来自 {successful_terms} 个条目)\")\n",
    "                        all_genes.update(file_genes)\n",
    "                        successful_files.append(file_path)\n",
    "                    else:\n",
    "                        print(\"该文件中未找到基因信息\")\n",
    "                        failed_files.append((file_path, \"无基因信息\"))\n",
    "                else:\n",
    "                    print(f\"数据不是字典格式: {type(data)}\")\n",
    "                    failed_files.append((file_path, \"数据格式错误\"))\n",
    "            else:\n",
    "                print(\"所有读取方法都失败\")\n",
    "                failed_files.append((file_path, \"读取失败\"))\n",
    "                \n",
    "        else:\n",
    "            print(f\"文件不存在: {file_path}\")\n",
    "            failed_files.append((file_path, \"文件不存在\"))\n",
    "    \n",
    "    return all_genes, successful_files, failed_files\n",
    "\n",
    "# 提取基因\n",
    "print(\"正在从fold数据中提取基因...\")\n",
    "benchmark_genes, successful_files, failed_files = extract_genes_from_fold_data()\n",
    "\n",
    "if benchmark_genes:\n",
    "    print(f\"\\n=== 提取结果总结 ===\")\n",
    "    print(f\"总共找到 {len(benchmark_genes)} 个独特基因\")\n",
    "    print(f\"成功处理的文件: {len(successful_files)}\")\n",
    "    print(f\"失败的文件: {len(failed_files)}\")\n",
    "    \n",
    "    # 显示一些示例基因\n",
    "    sample_genes = sorted(list(benchmark_genes))[:10]\n",
    "    print(f\"示例基因: {sample_genes}\")\n",
    "    \n",
    "    # 保存基因列表\n",
    "    with open('benchmark_gene_list_complete.txt', 'w') as f:\n",
    "        for gene in sorted(benchmark_genes):\n",
    "            f.write(f\"{gene}\\n\")\n",
    "    \n",
    "    print(\"\\n基因列表已保存到 benchmark_gene_list_complete.txt\")\n",
    "    \n",
    "    # 按文件类型统计\n",
    "    print(f\"\\n=== 成功提取来源 ===\")\n",
    "    go_files = 0\n",
    "    omim_files = 0\n",
    "    for file_path in successful_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"  ✓ {filename}\")\n",
    "        if 'omim' in filename.lower():\n",
    "            omim_files += 1\n",
    "        else:\n",
    "            go_files += 1\n",
    "    \n",
    "    print(f\"\\nGO文件: {go_files}, OMIM文件: {omim_files}\")\n",
    "    \n",
    "    # 显示失败的文件\n",
    "    if failed_files:\n",
    "        print(f\"\\n=== 失败的文件 ===\")\n",
    "        for file_path, reason in failed_files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"  ✗ {filename}: {reason}\")\n",
    "        \n",
    "else:\n",
    "    print(\"未能提取到基因信息，可能需要检查其他文件位置\")\n",
    "    \n",
    "    # 显示所有失败原因\n",
    "    print(\"\\n失败详情:\")\n",
    "    for file_path, reason in failed_files:\n",
    "        print(f\"  {os.path.basename(file_path)}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb326c8",
   "metadata": {},
   "source": [
    "### Convert gene ID into gene name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0779a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygene\n",
    "import pandas as pd\n",
    "\n",
    "def convert_entrez_to_symbols(entrez_file):\n",
    "    \"\"\"将Entrez ID批量转换为基因符号\"\"\"\n",
    "    \n",
    "    # 读取基因列表\n",
    "    with open(entrez_file, 'r') as f:\n",
    "        entrez_ids = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"准备转换 {len(entrez_ids)} 个Entrez ID\")\n",
    "    \n",
    "    # 初始化mygene\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    \n",
    "    # 批量转换（避免API限制）\n",
    "    batch_size = 1000\n",
    "    gene_mapping = {}\n",
    "    failed_ids = []\n",
    "    \n",
    "    for i in range(0, len(entrez_ids), batch_size):\n",
    "        batch = entrez_ids[i:i+batch_size]\n",
    "        batch_num = i//batch_size + 1\n",
    "        total_batches = (len(entrez_ids)-1)//batch_size + 1\n",
    "        \n",
    "        print(f\"处理批次 {batch_num}/{total_batches} ({len(batch)} 个基因)\")\n",
    "        \n",
    "        try:\n",
    "            results = mg.querymany(\n",
    "                batch, \n",
    "                scopes='entrezgene', \n",
    "                fields='symbol,entrezgene,name', \n",
    "                species='human',\n",
    "                returnall=True\n",
    "            )\n",
    "            \n",
    "            # 处理成功的结果\n",
    "            for result in results['out']:\n",
    "                if 'symbol' in result and 'entrezgene' in result:\n",
    "                    entrez_id = str(result['entrezgene'])\n",
    "                    symbol = result['symbol']\n",
    "                    gene_name = result.get('name', '')\n",
    "                    gene_mapping[entrez_id] = {\n",
    "                        'symbol': symbol,\n",
    "                        'name': gene_name\n",
    "                    }\n",
    "                elif 'query' in result:\n",
    "                    # 记录失败的ID\n",
    "                    failed_ids.append(result['query'])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"批次 {batch_num} 转换失败: {e}\")\n",
    "            failed_ids.extend(batch)\n",
    "    \n",
    "    print(f\"\\n转换完成！\")\n",
    "    print(f\"成功转换: {len(gene_mapping)} 个基因\")\n",
    "    print(f\"转换失败: {len(failed_ids)} 个基因\")\n",
    "    print(f\"转换成功率: {len(gene_mapping)/len(entrez_ids)*100:.1f}%\")\n",
    "    \n",
    "    return gene_mapping, failed_ids\n",
    "\n",
    "# 执行转换\n",
    "print(\"开始基因ID转换...\")\n",
    "gene_mapping, failed_ids = convert_entrez_to_symbols('benchmark_gene_list_complete.txt')\n",
    "\n",
    "if gene_mapping:\n",
    "    # 保存详细映射表\n",
    "    mapping_df = pd.DataFrame([\n",
    "        {\n",
    "            'entrez_id': entrez_id,\n",
    "            'gene_symbol': info['symbol'], \n",
    "            'gene_name': info['name']\n",
    "        }\n",
    "        for entrez_id, info in gene_mapping.items()\n",
    "    ])\n",
    "    \n",
    "    mapping_df.to_csv('benchmark_gene_mapping.csv', index=False)\n",
    "    print(f\"详细映射表已保存到 benchmark_gene_mapping.csv\")\n",
    "    \n",
    "    # 保存基因符号列表\n",
    "    symbols = sorted([info['symbol'] for info in gene_mapping.values()])\n",
    "    with open('benchmark_gene_symbols.txt', 'w') as f:\n",
    "        for symbol in symbols:\n",
    "            f.write(f\"{symbol}\\n\")\n",
    "    \n",
    "    print(f\"基因符号列表已保存到 benchmark_gene_symbols.txt\")\n",
    "    \n",
    "    # 显示一些示例\n",
    "    print(f\"\\n示例转换结果:\")\n",
    "    for i, (entrez_id, info) in enumerate(list(gene_mapping.items())[:10]):\n",
    "        print(f\"  {entrez_id} -> {info['symbol']} ({info['name'][:50]}...)\")\n",
    "    \n",
    "    # 保存失败的ID（供后续处理）\n",
    "    if failed_ids:\n",
    "        with open('failed_entrez_ids.txt', 'w') as f:\n",
    "            for failed_id in failed_ids:\n",
    "                f.write(f\"{failed_id}\\n\")\n",
    "        print(f\"\\n{len(failed_ids)} 个失败的ID已保存到 failed_entrez_ids.txt\")\n",
    "\n",
    "# 生成总结报告\n",
    "print(f\"\\n=== 基准测试基因库总结 ===\")\n",
    "print(f\"原始Entrez ID数量: 6544\")\n",
    "print(f\"成功转换的基因符号: {len(gene_mapping)}\")\n",
    "print(f\"最终可用基因数量: {len(gene_mapping)}\")\n",
    "print(f\"基因来源: GO (8个文件) + OMIM (4个文件)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab3daa",
   "metadata": {},
   "source": [
    "### map into our dataset's gene list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe7f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "merfish_adata = sc.read_h5ad('/media/dang/Omics/data/spot_level/bento/merfish_processed.h5ad')\n",
    "merfish_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmx_data = pd.read_pickle('/media/dang/Omics/data/spot_level/CosMx/spot_dataframe.pkl')\n",
    "CosMx_gene = np.unique(cosmx_data['gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_compare_genes():\n",
    "    \"\"\"从实际数据中提取基因并与基准比较\"\"\"\n",
    "    \n",
    "    # 读取基准测试基因\n",
    "    with open('benchmark_gene_symbols.txt', 'r') as f:\n",
    "        benchmark_genes = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "    print(f\"基准测试基因总数: {len(benchmark_genes)}\")\n",
    "    \n",
    "    # === 方法1: 如果你有merfish_adata对象 ===\n",
    "    try:\n",
    "        # 假设你的merfish_adata已经加载\n",
    "        merfish_genes = list(merfish_adata.var_names)\n",
    "        # 过滤掉non-target基因\n",
    "        merfish_genes = [g for g in merfish_genes if not g.startswith('notarget')]\n",
    "        print(f\"merFISH基因数 (过滤后): {len(merfish_genes)}\")\n",
    "    except:\n",
    "        print(\"请先加载merfish_adata\")\n",
    "        merfish_genes = []\n",
    "    \n",
    "    # === 方法2: 如果你有CosMx数据文件 ===\n",
    "    try:\n",
    "        # 从pickle文件中读取\n",
    "        cosmx_data = pd.read_pickle('/media/dang/Omics/data/spot_level/CosMx/spot_dataframe.pkl')\n",
    "        cosmx_genes = list(np.unique(cosmx_data['gene']))\n",
    "        print(f\"CosMx基因数: {len(cosmx_genes)}\")\n",
    "    except:\n",
    "        print(\"无法读取CosMx数据文件\")\n",
    "        cosmx_genes = []\n",
    "    \n",
    "    # 进行匹配分析\n",
    "    if merfish_genes or cosmx_genes:\n",
    "        # 转为大写进行匹配\n",
    "        benchmark_upper = set(g.upper() for g in benchmark_genes)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if merfish_genes:\n",
    "            merfish_upper = set(g.upper() for g in merfish_genes)\n",
    "            merfish_overlap = merfish_upper.intersection(benchmark_upper)\n",
    "            \n",
    "            print(f\"\\n=== merFISH 匹配结果 ===\")\n",
    "            print(f\"merFISH基因: {len(merfish_genes)}\")\n",
    "            print(f\"与基准重叠: {len(merfish_overlap)}\")\n",
    "            print(f\"重叠率: {len(merfish_overlap)/len(benchmark_upper)*100:.1f}%\")\n",
    "            \n",
    "            # 显示一些重叠的基因\n",
    "            sample_overlap = sorted(list(merfish_overlap))[:10]\n",
    "            print(f\"重叠基因示例: {sample_overlap}\")\n",
    "            \n",
    "            results['merfish'] = {\n",
    "                'total': len(merfish_genes),\n",
    "                'overlap': len(merfish_overlap),\n",
    "                'overlap_genes': sorted(merfish_overlap)\n",
    "            }\n",
    "        \n",
    "        if cosmx_genes:\n",
    "            cosmx_upper = set(g.upper() for g in cosmx_genes)\n",
    "            cosmx_overlap = cosmx_upper.intersection(benchmark_upper)\n",
    "            \n",
    "            print(f\"\\n=== CosMx 匹配结果 ===\")\n",
    "            print(f\"CosMx基因: {len(cosmx_genes)}\")\n",
    "            print(f\"与基准重叠: {len(cosmx_overlap)}\")\n",
    "            print(f\"重叠率: {len(cosmx_overlap)/len(benchmark_upper)*100:.1f}%\")\n",
    "            \n",
    "            # 显示一些重叠的基因\n",
    "            sample_overlap = sorted(list(cosmx_overlap))[:10]\n",
    "            print(f\"重叠基因示例: {sample_overlap}\")\n",
    "            \n",
    "            results['cosmx'] = {\n",
    "                'total': len(cosmx_genes),\n",
    "                'overlap': len(cosmx_overlap),\n",
    "                'overlap_genes': sorted(cosmx_overlap)\n",
    "            }\n",
    "        \n",
    "        # 如果两个数据集都有，比较它们\n",
    "        if merfish_genes and cosmx_genes:\n",
    "            merfish_upper = set(g.upper() for g in merfish_genes)\n",
    "            cosmx_upper = set(g.upper() for g in cosmx_genes)\n",
    "            \n",
    "            both_overlap = merfish_upper.intersection(cosmx_upper)\n",
    "            three_way_overlap = merfish_overlap.intersection(cosmx_overlap)\n",
    "            \n",
    "            print(f\"\\n=== 数据集间比较 ===\")\n",
    "            print(f\"merFISH与CosMx重叠: {len(both_overlap)}\")\n",
    "            print(f\"三方重叠(都与基准匹配): {len(three_way_overlap)}\")\n",
    "            \n",
    "            results['comparison'] = {\n",
    "                'merfish_cosmx_overlap': len(both_overlap),\n",
    "                'three_way_overlap': len(three_way_overlap),\n",
    "                'three_way_genes': sorted(three_way_overlap)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    else:\n",
    "        print(\"没有可用的基因数据进行比较\")\n",
    "        return None\n",
    "\n",
    "# 运行分析\n",
    "results = extract_and_compare_genes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020fba5",
   "metadata": {},
   "source": [
    "### Save matched genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_matched_genes_for_embedding():\n",
    "    \"\"\"保存用于生成embedding的基因列表\"\"\"\n",
    "    \n",
    "    # 读取基准测试基因\n",
    "    with open('benchmark_gene_symbols.txt', 'r') as f:\n",
    "        benchmark_genes = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "    # 从你的数据中提取基因（假设你已经运行了之前的代码）\n",
    "    try:\n",
    "        # merFISH基因\n",
    "        merfish_genes = list(merfish_adata.var_names)\n",
    "        merfish_genes_filtered = [g for g in merfish_genes if not g.startswith('notarget')]\n",
    "        \n",
    "        # CosMx基因  \n",
    "        cosmx_data = pd.read_pickle('/media/dang/Omics/data/spot_level/CosMx/spot_dataframe.pkl')\n",
    "        cosmx_genes = list(np.unique(cosmx_data['gene']))\n",
    "        \n",
    "        # 计算重叠\n",
    "        benchmark_upper = set(g.upper() for g in benchmark_genes)\n",
    "        merfish_upper = set(g.upper() for g in merfish_genes_filtered)\n",
    "        cosmx_upper = set(g.upper() for g in cosmx_genes)\n",
    "        \n",
    "        merfish_overlap = merfish_upper.intersection(benchmark_upper)\n",
    "        cosmx_overlap = cosmx_upper.intersection(benchmark_upper)\n",
    "        \n",
    "        # 保存各自的重叠基因\n",
    "        with open('merfish_benchmark_genes.txt', 'w') as f:\n",
    "            for gene in sorted(merfish_overlap):\n",
    "                f.write(f\"{gene}\\n\")\n",
    "        \n",
    "        with open('cosmx_benchmark_genes.txt', 'w') as f:\n",
    "            for gene in sorted(cosmx_overlap):\n",
    "                f.write(f\"{gene}\\n\")\n",
    "        \n",
    "        # 保存合并的基因列表（用于完整基准测试）\n",
    "        combined_genes = merfish_overlap.union(cosmx_overlap)\n",
    "        with open('combined_benchmark_genes.txt', 'w') as f:\n",
    "            for gene in sorted(combined_genes):\n",
    "                f.write(f\"{gene}\\n\")\n",
    "        \n",
    "        # 找出三方重叠的基因（可以进行直接比较）\n",
    "        three_way_overlap = merfish_overlap.intersection(cosmx_overlap)\n",
    "        with open('three_way_overlap_genes.txt', 'w') as f:\n",
    "            for gene in sorted(three_way_overlap):\n",
    "                f.write(f\"{gene}\\n\")\n",
    "        \n",
    "        print(\"=== 基因列表已保存 ===\")\n",
    "        print(f\"✅ merfish_benchmark_genes.txt: {len(merfish_overlap)} 个基因\")\n",
    "        print(f\"✅ cosmx_benchmark_genes.txt: {len(cosmx_overlap)} 个基因\") \n",
    "        print(f\"✅ combined_benchmark_genes.txt: {len(combined_genes)} 个基因\")\n",
    "        print(f\"✅ three_way_overlap_genes.txt: {len(three_way_overlap)} 个基因\")\n",
    "        \n",
    "        return {\n",
    "            'merfish_genes': sorted(merfish_overlap),\n",
    "            'cosmx_genes': sorted(cosmx_overlap),\n",
    "            'combined_genes': sorted(combined_genes),\n",
    "            'three_way_genes': sorted(three_way_overlap)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理基因数据时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "# 保存基因列表\n",
    "gene_lists = save_matched_genes_for_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed042b",
   "metadata": {},
   "source": [
    "### filter new gene list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90328f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_subsets_with_compatibility():\n",
    "    \"\"\"处理pandas版本兼容性问题的测试子集创建\"\"\"\n",
    "    \n",
    "    import pickle\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 读取你的基因列表\n",
    "    with open('combined_benchmark_genes.txt', 'r') as f:\n",
    "        target_genes = set(line.strip().upper() for line in f)\n",
    "    \n",
    "    print(f\"目标基因数量: {len(target_genes)}\")\n",
    "    \n",
    "    # 读取基因映射\n",
    "    gene_mapping = pd.read_csv('benchmark_gene_mapping.csv')\n",
    "    entrez_to_symbol = dict(zip(gene_mapping['entrez_id'].astype(str), \n",
    "                               gene_mapping['gene_symbol'].str.upper()))\n",
    "    \n",
    "    # 原始基准测试文件\n",
    "    benchmark_files = [\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold1_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold2_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_cv_fold3_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/GO/go_holdout_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold1_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold2_dict.pkl\",\n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_cv_fold3_dict.pkl\", \n",
    "        \"../gene-embedding-benchmarks/bin/gene_level/OMIM/omim_holdout_dict.pkl\"\n",
    "    ]\n",
    "    \n",
    "    os.makedirs('test_subsets', exist_ok=True)\n",
    "    \n",
    "    subset_stats = []\n",
    "    \n",
    "    for file_path in benchmark_files:\n",
    "        if os.path.exists(file_path):\n",
    "            filename = os.path.basename(file_path)\n",
    "            print(f\"\\n处理 {filename}...\")\n",
    "            \n",
    "            try:\n",
    "                # 方法1: 直接读取\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    original_data = pickle.load(f)\n",
    "                \n",
    "            except Exception as e1:\n",
    "                print(f\"直接读取失败: {e1}\")\n",
    "                \n",
    "                try:\n",
    "                    # 方法2: 使用protocol=2\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        original_data = pickle.load(f)\n",
    "                        \n",
    "                except Exception as e2:\n",
    "                    print(f\"Protocol=2读取失败: {e2}\")\n",
    "                    \n",
    "                    try:\n",
    "                        # 方法3: 使用pd.read_pickle\n",
    "                        original_data = pd.read_pickle(file_path)\n",
    "                        \n",
    "                    except Exception as e3:\n",
    "                        print(f\"pd.read_pickle读取失败: {e3}\")\n",
    "                        \n",
    "                        # 方法4: 跳过这个文件，记录问题\n",
    "                        print(f\"❌ 跳过 {filename} - 无法读取\")\n",
    "                        continue\n",
    "            \n",
    "            # 如果成功读取，进行处理\n",
    "            try:\n",
    "                subset_data = {}\n",
    "                total_samples = 0\n",
    "                kept_samples = 0\n",
    "                \n",
    "                for term, data in original_data.items():\n",
    "                    # 处理不同的数据格式\n",
    "                    if isinstance(data, pd.DataFrame):\n",
    "                        df = data\n",
    "                    elif hasattr(data, 'to_frame'):\n",
    "                        df = data.to_frame()\n",
    "                    elif isinstance(data, dict):\n",
    "                        df = pd.DataFrame(data)\n",
    "                    else:\n",
    "                        # 尝试转换为DataFrame\n",
    "                        try:\n",
    "                            df = pd.DataFrame(data)\n",
    "                        except:\n",
    "                            print(f\"无法处理term {term}的数据格式\")\n",
    "                            continue\n",
    "                    \n",
    "                    if 'gene' in df.columns:\n",
    "                        total_samples += len(df)\n",
    "                        \n",
    "                        # 创建新的DataFrame避免版本问题\n",
    "                        new_df = pd.DataFrame({\n",
    "                            'gene': df['gene'].values,\n",
    "                            'result': df['result'].values if 'result' in df.columns else [1] * len(df)\n",
    "                        })\n",
    "                        \n",
    "                        # 转换基因ID\n",
    "                        new_df['gene_symbol'] = new_df['gene'].astype(str).map(entrez_to_symbol)\n",
    "                        \n",
    "                        # 筛选目标基因\n",
    "                        mask = new_df['gene_symbol'].isin(target_genes)\n",
    "                        subset_df = new_df[mask][['gene', 'result']].copy()\n",
    "                        \n",
    "                        if len(subset_df) > 0:\n",
    "                            subset_data[term] = subset_df\n",
    "                            kept_samples += len(subset_df)\n",
    "                \n",
    "                # 保存子集文件\n",
    "                if subset_data:\n",
    "                    output_filename = filename.replace('.pkl', '_subset.pkl')\n",
    "                    output_path = os.path.join('test_subsets', output_filename)\n",
    "                    \n",
    "                    with open(output_path, 'wb') as f:\n",
    "                        pickle.dump(subset_data, f, protocol=2)  # 使用兼容性更好的protocol\n",
    "                    \n",
    "                    subset_stats.append({\n",
    "                        'file': output_filename,\n",
    "                        'terms': len(subset_data),\n",
    "                        'original_samples': total_samples,\n",
    "                        'kept_samples': kept_samples,\n",
    "                        'retention_rate': f\"{kept_samples/total_samples*100:.1f}%\" if total_samples > 0 else \"0%\"\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"✅ {output_filename}: {len(subset_data)} terms, {kept_samples}/{total_samples} samples\")\n",
    "                else:\n",
    "                    print(f\"⚠️ {filename}: 没有匹配的数据\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ 处理 {filename} 的数据时出错: {e}\")\n",
    "    \n",
    "    # 打印统计摘要\n",
    "    print(f\"\\n=== 子集创建摘要 ===\")\n",
    "    print(f\"成功创建 {len(subset_stats)} 个测试子集文件\")\n",
    "    \n",
    "    for stat in subset_stats:\n",
    "        print(f\"  {stat['file']}: {stat['terms']} terms, {stat['retention_rate']} retention\")\n",
    "    \n",
    "    return subset_stats\n",
    "\n",
    "# 运行修复版本\n",
    "subset_stats = prepare_test_subsets_with_compatibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5e0b5",
   "metadata": {},
   "source": [
    "## Generate gene embeddings for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48334efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接按照命令行脚本写死推理配置，避免 Notebook 再次解析 CLI\n",
    "args = Namespace(\n",
    "    ckpt_path=\"/path/to/your/checkpoint.ckpt\",  # TODO: 替换成实际的checkpoint路径\n",
    "    dataset_name=\"seqfish\",  # 根据实际情况修改\n",
    "    target_dataset=\"seqfish\",\n",
    "    gene_pct=100,\n",
    "    linear_hidden_dim='256',\n",
    "    radius=20,\n",
    "    max_points=20,\n",
    "    config='/media/dang/Omics/omics/configs/bert_config_5-12.json',\n",
    "    seed=42,\n",
    "    f=None,\n",
    "    split_slice=None,\n",
    "    target_width=6724,\n",
    "    target_height=5885,\n",
    "    x_min=None,\n",
    "    x_max=None,\n",
    "    y_min=None,\n",
    "    y_max=None,\n",
    "    sample_ratio=1.0,\n",
    "    merge_threshold=0.5,\n",
    "    percentile=70,\n",
    "    gene_list_path=\"/media/dang/Omics/omics/gene_level/function/preprocess/cosmx_benchmark_genes.txt\"\n",
    ")\n",
    "\n",
    "print('ckpt_path:', args.ckpt_path)\n",
    "print(f\"Dataset: {args.dataset_name}, radius: {args.radius}, max_points: {args.max_points}\")\n",
    "\n",
    "# 构建cache路径\n",
    "if args.dataset_name == 'cosmx_lung5_rep1':\n",
    "    cache_path = f'/media/dang/Omics/omics/baseline/cached_spot_data/spot_input_spot_sampled_{args.dataset_name}_42_{args.gene_pct}_{args.radius}_{args.max_points}_0.1.h5'\n",
    "else:\n",
    "    cache_path = f'/media/dang/Omics/omics/baseline/cached_spot_data/spot_input_spot_all_{args.dataset_name}_42_{args.gene_pct}_{args.radius}_{args.max_points}.h5'\n",
    "\n",
    "print(\"Processing gene-level embeddings from gene list -> CSV format\")\n",
    "emb_file, genelist_file = process_single_gpu_gene_embeddings(\n",
    "    args, args.gene_list_path, args.target_dataset, cache_path\n",
    ")\n",
    "\n",
    "# 设置数据集特定参数\n",
    "args.linear_hidden_dim = '256'\n",
    "args.radius = 20\n",
    "args.max_points = 20\n",
    "args.config = '/media/dang/Omics/omics/configs/bert_config_5-12.json'\n",
    "print(f\"Dataset: {args.dataset_name}, radius: {args.radius}, max_points: {args.max_points}\")\n",
    "\n",
    "# 构建cache路径\n",
    "if args.dataset_name == 'cosmx_lung5_rep1':\n",
    "    cache_path = f'/media/dang/Omics/omics/baseline/cached_spot_data/spot_input_spot_sampled_{args.dataset_name}_42_{args.gene_pct}_{args.radius}_{args.max_points}_0.1.h5'\n",
    "else:\n",
    "    cache_path = f'/media/dang/Omics/omics/baseline/cached_spot_data/spot_input_spot_all_{args.dataset_name}_42_{args.gene_pct}_{args.radius}_{args.max_points}.h5'\n",
    "\n",
    "print(\"Processing gene-level embeddings from gene list -> CSV format\")\n",
    "emb_file, genelist_file = process_single_gpu_gene_embeddings(\n",
    "    args, args.gene_list_path, args.target_dataset, cache_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c8c71",
   "metadata": {},
   "source": [
    "## Convert gene name list to gene ID list using benchmark gene mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec722fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接按照配置写死参数，避免 Notebook 再次解析 CLI\n",
    "args = Namespace(\n",
    "    genelist_file='/media/dang/Omics/omics/gene_level/function/preprocess/gene_embeddings_csv/merfish_gene100pct_Spotformer_genelist.txt',\n",
    "    mapping_file=\"/media/dang/Omics/omics/gene_level/function/preprocess/benchmark_gene_mapping.csv\",\n",
    "    output_file='/media/dang/Omics/omics/gene_level/function/preprocess/gene_embeddings_csv/Spotformer_genelist2.txt'\n",
    ")\n",
    "\n",
    "# 如果没有指定输出文件，自动生成\n",
    "if args.output_file is None:\n",
    "    # 从genelist文件名生成对应的gene ID文件名\n",
    "    base_name = os.path.splitext(args.genelist_file)[0]\n",
    "    if base_name.endswith('_genelist'):\n",
    "        base_name = base_name[:-9]  # 移除'_genelist'\n",
    "    args.output_file = f\"{base_name}_geneid.txt\"\n",
    "\n",
    "print(f\"Converting gene names to IDs...\")\n",
    "print(f\"  Input: {args.genelist_file}\")\n",
    "print(f\"  Mapping: {args.mapping_file}\")\n",
    "print(f\"  Output: {args.output_file}\")\n",
    "\n",
    "gene_ids, matched_count, unmatched_genes = convert_gene_names_to_ids(\n",
    "    args.genelist_file, args.mapping_file, args.output_file\n",
    ")\n",
    "\n",
    "print(f\"\\nDone! Generated {len(gene_ids)} gene IDs with {matched_count} matches.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee016c",
   "metadata": {},
   "source": [
    "## Merge gene embeddings from different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接按照配置写死参数，避免 Notebook 再次解析 CLI\n",
    "args = Namespace(\n",
    "    emb_file1=\"/media/dang/Omics/omics/gene_level/function/preprocess/gene_embeddings_csv/cosmx_lung5_rep1_gene20pct_Spotformer_emb.csv\",\n",
    "    emb_file2=\"/media/dang/Omics/omics/gene_level/function/preprocess/gene_embeddings_csv/merfish_gene100pct_Spotformer_emb.csv\",\n",
    "    gene_file1=\"/media/dang/Omics/omics/gene_level/function/preprocess/gene_embeddings_csv/Spotformer_genelist.txt\",\n",
    "    gene_file2=\"/media/dang/Omics/omics/gene_level/function/preprocess/gene_embeddings_csv/Spotformer_genelist2.txt\",\n",
    "    output_dir=\"/media/dang/Omics/omics/gene_level/function/preprocess/gene_embeddings_csv/\",\n",
    "    output_prefix=\"merged\"\n",
    ")\n",
    "\n",
    "# 创建输出文件路径\n",
    "merged_emb_file = os.path.join(args.output_dir, f\"{args.output_prefix}_Spotformer_emb.csv\")\n",
    "merged_gene_file = os.path.join(args.output_dir, f\"{args.output_prefix}_Spotformer_genelist.txt\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MERGING EMBEDDING FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 合并embedding文件\n",
    "merged_embeddings = merge_embedding_csvs(\n",
    "    args.emb_file1, args.emb_file2, merged_emb_file\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MERGING GENE LIST FILES\") \n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 合并gene list文件\n",
    "merged_genes = merge_gene_lists(\n",
    "    args.gene_file1, args.gene_file2, merged_gene_file\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 验证一致性\n",
    "if len(merged_genes) == len(merged_embeddings):\n",
    "    print(f\"✓ Consistency check passed: {len(merged_genes)} genes match {len(merged_embeddings)} embeddings\")\n",
    "else:\n",
    "    print(f\"✗ Consistency check failed: {len(merged_genes)} genes vs {len(merged_embeddings)} embeddings\")\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  Merged embeddings: {merged_emb_file}\")\n",
    "print(f\"  Merged gene list: {merged_gene_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
